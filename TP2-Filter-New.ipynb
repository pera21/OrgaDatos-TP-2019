{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom sklearn.cluster import KMeans\\n\\nfrom sklearn.svm import SVC\\nfrom sklearn.multioutput import MultiOutputClassifier\\nfrom sklearn.ensemble import GradientBoostingClassifier\\nfrom sklearn.tree import DecisionTreeClassifier\\nfrom sklearn.neighbors import KNeighborsClassifier\\nfrom sklearn.naive_bayes import GaussianNB\\nfrom sklearn.ensemble import RandomForestClassifier\\nfrom sklearn.linear_model import LogisticRegression\\nimport xgboost as xgb\\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime as datetime\n",
    "import datetime \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maximiliano/miniconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2785: DtypeWarning: Columns (13,20) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n",
      "/home/maximiliano/miniconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2785: DtypeWarning: Columns (16,17) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n",
      "/home/maximiliano/miniconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2785: DtypeWarning: Columns (4,15) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "events = pd.read_csv(\"events.csv.gzip\", compression='gzip')\n",
    "clicks = pd.read_csv(\"clicks.csv.gzip\", compression='gzip')\n",
    "installs = pd.read_csv(\"installs.csv.gzip\", compression='gzip')\n",
    "\n",
    "pd.set_option('display.max_columns', 50)\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformo a datetime la fecha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "installs['created'] = pd.to_datetime(installs['created'])\n",
    "events['date'] = pd.to_datetime(events['date'])\n",
    "clicks['created'] = pd.to_datetime(clicks['created'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "installs['date_seconds_level'] = installs['created'].astype('datetime64[s]')\n",
    "events['date_seconds_level'] = events['date'].astype('datetime64[s]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agrego diferencias entre installs de la misma aplicacion para el mismo usuario (para borrar posibles errores de la tracking platform luego)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "installs['seconds_to_prev_install'] = installs.sort_values(['ref_hash', 'created'], ascending=True).groupby(['ref_hash', 'application_id'])['created'].diff()\n",
    "installs['seconds_to_prev_install'] = installs['seconds_to_prev_install']/np.timedelta64(1, 's')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elimino de installs las instalaciones con menos de un segundo de diferencia para el mismo usuario y misma applicacion (las suponemos error de tracking platform) (22077 installs menos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#installs = installs.loc[(installs['seconds_to_prev_install'] > 1) | pd.isnull(installs['seconds_to_prev_install'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Busco los installs que no se corresponden con un evento (un install deberia tener un evento en la tabla events para esa fecha). Para ellos genero un evento (774 eventos nuevos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "installs_with_events = pd.merge(events, installs, on=['ref_hash', 'date_seconds_level', 'application_id'])\n",
    "installs_with_events['install_with_event'] = True\n",
    "installs = pd.merge(installs, installs_with_events[['ref_hash', 'date_seconds_level', 'application_id', 'install_with_event']], on=['ref_hash', 'date_seconds_level', 'application_id'], how='left')\n",
    "installs.head()\n",
    "installs.loc[installs['install_with_event'] != True, 'install_with_event'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maximiliano/miniconda3/lib/python3.6/site-packages/ipykernel_launcher.py:2: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "#Creo el evento por cada install no correspondido (solo los installs implicitos tienen un evento correspondiente)\n",
    "events = pd.concat([events, installs.loc[(installs['install_with_event'] == False) & (installs['implicit'] == True), ['ref_hash', 'created', 'application_id']]], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "events.loc[pd.isnull(events['date']), 'date'] = pd.to_datetime(events['created'])\n",
    "events.drop(['created'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agrego columnas a events, clicks y a installs para poder clasificar despues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "installs['install_day'] = installs['created'].dt.day\n",
    "events['event_day'] = events['date'].dt.day\n",
    "clicks['click_day'] = clicks['created'].dt.day"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creando el dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separo los dataframe para obtener las distintas ventanas con las cuales entrenar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-04-18 00:00:01.560000 2019-04-26 23:59:58.788000\n",
      "2019-04-18 00:00:00.027000 2019-04-26 23:59:59.881000\n",
      "2019-04-12 00:00:01.981000 2019-04-26 23:59:22.065000\n"
     ]
    }
   ],
   "source": [
    "print(installs['creat6794880020077884416.000  ed'].min(), installs['created'].max())\n",
    "print(events['date'].min(), events['date'].max())\n",
    "print(clicks['created'].min(), clicks['created'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_installs = {}\n",
    "df_events = {}\n",
    "df_clicks = {}\n",
    "devices = {}\n",
    "for i in range(18, 23):\n",
    "    windowStartDate = datetime.datetime(2019, 4, i)\n",
    "    windowEndDate = datetime.datetime(2019, 4, i + 3)\n",
    "    df_installs[i - 18] = installs.loc[(installs['created'] >= windowStartDate) & (windowEndDate >= installs['created'])]\n",
    "    df_events[i - 18] = events.loc[(events['date'] >= windowStartDate) & (windowEndDate >= events['date'])]\n",
    "    df_clicks[i - 18] = clicks.loc[(clicks['created'] >= windowStartDate) & (windowEndDate >= clicks['created'])]\n",
    "    \n",
    "    #Creo un dataframe de dispositivos con los dispositivos que convierten en la proxima ventana \n",
    "    next_window_installs = installs.loc[(installs['created'] >= datetime.datetime(2019, 4, i + 3)) & (datetime.datetime(2019, 4, i + 5) >= installs['created'])]\n",
    "    devices[i - 18] = pd.DataFrame(next_window_installs['ref_hash'].unique())\n",
    "    \n",
    "    #Agrego los segundos a la proxima conversion (desde el inicio de la ventana)\n",
    "    devices[i - 18].columns = ['ref_hash']\n",
    "    devices[i - 18] = pd.merge(devices[i - 18], next_window_installs.groupby('ref_hash')['created'].min().reset_index(), on='ref_hash')\n",
    "    devices[i - 18].rename(columns = {'created':'seconds_to_conversion'}, inplace=True)\n",
    "    devices[i - 18]['seconds_to_conversion'] = (devices[i - 18]['seconds_to_conversion'] - windowStartDate)/np.timedelta64(1, 's')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agrego features para cada dataframe de devices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, 5):\n",
    "    #Porcentaje de installs implicitos y no implicitos en el periodo + total installs en el periodo\n",
    "    #devices[i] = pd.merge(devices[i], df_installs[i].groupby('ref_hash')['created'].count().reset_index(), on='ref_hash')\n",
    "    #devices[i].rename(columns = {'created':'total_installs'}, inplace=True\n",
    "    installs_by_implicit = pd.pivot_table(df_installs[i], index='ref_hash', columns='implicit', values='created', aggfunc='count', fill_value=0, margins=True).reset_index()\n",
    "    installs_by_implicit.drop(installs_by_implicit.tail(1).index, inplace=True) #Elimino el margin creado por pivot_table\n",
    "    installs_by_implicit.columns = ['ref_hash', 'installs_not_implicit', 'installs_implicit', 'total_installs']\n",
    "    installs_by_implicit['ref_hash'] = installs_by_implicit['ref_hash'].astype('int64')\n",
    "    installs_by_implicit['installs_not_implicit'] = installs_by_implicit['installs_not_implicit']/installs_by_implicit['total_installs']\n",
    "    installs_by_implicit['installs_implicit'] = installs_by_implicit['installs_implicit']/installs_by_implicit['total_installs']\n",
    "    devices[i] = pd.merge(devices[i], installs_by_implicit, on='ref_hash')\n",
    "    \n",
    "    #Porcentaje de installs por dia en el periodo\n",
    "    installs_by_day = pd.pivot_table(df_installs[i], index='ref_hash', columns='install_day', values='created', aggfunc='count', fill_value=0, margins=True).reset_index()\n",
    "    installs_by_day.drop(installs_by_day.tail(1).index, inplace=True) #Elimino el margin creado por pivot_table\n",
    "    installs_by_day.columns = ['ref_hash', 'installs_day_1', 'installs_day_2', 'installs_day_3', 'total_installs']\n",
    "    installs_by_day['ref_hash'] = installs_by_day['ref_hash'].astype('int64')\n",
    "    installs_by_day['installs_day_1'] = installs_by_day['installs_day_1']/installs_by_day['total_installs']\n",
    "    installs_by_day['installs_day_2'] = installs_by_day['installs_day_2']/installs_by_day['total_installs']\n",
    "    installs_by_day['installs_day_3'] = installs_by_day['installs_day_3']/installs_by_day['total_installs']\n",
    "    devices[i] = pd.merge(devices[i], installs_by_day[['ref_hash', 'installs_day_1', 'installs_day_2', 'installs_day_3']], on='ref_hash')\n",
    "    \n",
    "    #Porcentaje de eventos por dia en el periodo + total eventos\n",
    "    #devices[i] = pd.merge(devices[i], df_events[i].groupby('ref_hash')['date'].count().reset_index(), on='ref_hash')\n",
    "    #devices[i].rename(columns = {'date':'total_events'}, inplace=True)\n",
    "    events_by_day = pd.pivot_table(df_events[i], index='ref_hash', columns='event_day', values='date', aggfunc='count', fill_value=0, margins=True).reset_index()\n",
    "    events_by_day.drop(events_by_day.tail(1).index, inplace=True) #Elimino el margin creado por pivot_table\n",
    "    events_by_day.columns = ['ref_hash', 'events_day_1', 'events_day_2', 'events_day_3', 'total_events']\n",
    "    events_by_day['ref_hash'] = events_by_day['ref_hash'].astype('int64')\n",
    "    events_by_day['events_day_1'] = events_by_day['events_day_1']/events_by_day['total_events']\n",
    "    events_by_day['events_day_2'] = events_by_day['events_day_2']/events_by_day['total_events']\n",
    "    events_by_day['events_day_3'] = events_by_day['events_day_3']/events_by_day['total_events']\n",
    "    devices[i] = pd.merge(devices[i], events_by_day, on='ref_hash')\n",
    "    \n",
    "    #Calculo el porcentaje de eventos hechos a traves de wifi (esto puede influir en un futuro install)\n",
    "    events_by_wifi = pd.pivot_table(df_events[i], index='ref_hash', columns='wifi', values='date', aggfunc='count', fill_value=0).reset_index()\n",
    "    events_by_wifi['events_on_wifi'] = events_by_wifi[True]/(events_by_wifi[True] + events_by_wifi[False])\n",
    "    devices[i] = pd.merge(devices[i], events_by_wifi[['ref_hash', 'events_on_wifi']], on='ref_hash')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
